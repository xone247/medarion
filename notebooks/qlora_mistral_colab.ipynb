{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Fine-tune Mistral 7B (Colab/Kaggle)\n",
        "\n",
        "Steps: install deps, mount Drive (optional), load our chat shards from `data/sft/chat_mistral/*.jsonl`, run SFTTrainer with LoRA, then save adapters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install transformers peft datasets accelerate bitsandbytes trl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob, json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "\n",
        "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "shards = glob.glob('data/sft/chat_mistral/*.jsonl')\n",
        "assert shards, 'No shards found; upload or generate via scripts/build_sft_dataset.py'\n",
        "\n",
        "def load_chat_dataset(paths):\n",
        "    records = []\n",
        "    for p in paths:\n",
        "        with open(p, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if not line.strip():\n",
        "                    continue\n",
        "                records.append(json.loads(line))\n",
        "    return Dataset.from_list(records)\n",
        "\n",
        "ds = load_chat_dataset(shards)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, load_in_4bit=True, device_map='auto', torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16, lora_alpha=32, target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n",
        "    lora_dropout=0.05, bias='none', task_type='CAUSAL_LM'\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='qlora-mistral', per_device_train_batch_size=1, gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4, fp16=True, logging_steps=10, num_train_epochs=1, save_strategy='no', report_to='none'\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model, tokenizer=tokenizer, train_dataset=ds, max_seq_length=2048,\n",
        "    args=args, dataset_text_field='text'\n",
        ")\n",
        "trainer.train()\n",
        "model.save_pretrained('qlora-mistral')\n",
        "tokenizer.save_pretrained('qlora-mistral')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
