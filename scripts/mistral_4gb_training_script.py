#!/usr/bin/env python3
"""
Mistral 7B Instruct v0.2 (4GB) Medarion Training Script
======================================================

This version uses the 4GB Mistral model - faster and more efficient for your platform!
"""

# Install packages
print("ğŸ“¦ Installing required packages...")
!pip install transformers torch accelerate datasets
print("âœ… Packages installed!")

import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from datasets import Dataset

# Configuration - Using 4GB Mistral model
MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.2"  # 4GB version
DATASET_PATH = "/kaggle/input/xone-finetuning-data"
OUTPUT_DIR = "/kaggle/working/medarion-mistral-4gb"

print("ğŸš€ Starting Medarion Training with 4GB Mistral...")
print(f"ğŸ“ Dataset path: {DATASET_PATH}")
print(f"ğŸ“ Output path: {OUTPUT_DIR}")

# Load model and tokenizer
print("ğŸ“¥ Loading Mistral 7B Instruct v0.2 (4GB model)...")
print("â³ This will download ~4GB model files...")
print("â³ Much faster download than 13GB version!")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
print("âœ… Tokenizer loaded!")

model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto")
print("âœ… Mistral 7B Instruct v0.2 model loaded!")
print(f"ğŸ“Š Model size: {model.num_parameters():,} parameters")
print("ğŸ¯ This 4GB model is optimized for instruction following!")

tokenizer.pad_token = tokenizer.eos_token
print("âœ… Pad token set!")

# Load training data
def load_data(file_path):
    print(f"ğŸ“‚ Loading data from: {file_path}")
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data

print("ğŸ“Š Loading training data...")
train_data = load_data(f"{DATASET_PATH}/train.jsonl")
print(f"âœ… Loaded {len(train_data):,} training records")

print("ğŸ“Š Loading validation data...")
val_data = load_data(f"{DATASET_PATH}/validation.jsonl")
print(f"âœ… Loaded {len(val_data):,} validation records")

# Format data
def format_instruction(example):
    if example["input"]:
        text = f"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}"
    else:
        text = f"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}"
    return {"text": text}

print("ğŸ”„ Formatting training data...")
train_dataset = Dataset.from_list(train_data).map(format_instruction)
print("âœ… Training data formatted!")

print("ğŸ”„ Formatting validation data...")
val_dataset = Dataset.from_list(val_data).map(format_instruction)
print("âœ… Validation data formatted!")

# Tokenize
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=2048, return_tensors="pt")

print("ğŸ”„ Tokenizing training data...")
train_dataset = train_dataset.map(tokenize_function, batched=True)
print("âœ… Training data tokenized!")

print("ğŸ”„ Tokenizing validation data...")
val_dataset = val_dataset.map(tokenize_function, batched=True)
print("âœ… Validation data tokenized!")

# Training setup
print("âš™ï¸ Setting up training configuration...")
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,
    per_device_train_batch_size=2,  # Can use larger batch size with 4GB model
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=2,  # Reduced since batch size is larger
    learning_rate=2e-5,
    warmup_steps=100,
    weight_decay=0.01,
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=1000,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    fp16=True,
    dataloader_pin_memory=False,
    remove_unused_columns=False,
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
)

print("âœ… Training setup complete!")

# Start training
print("ğŸ¯ Starting Medarion fine-tuning with 4GB model...")
print("â° This will take 4-6 hours (faster than 13GB version)...")
print("ğŸ“Š You'll see loss values decreasing over time...")
print("ğŸ”„ Training 3 epochs on 474,053 records...")

trainer.train()

print("âœ… Training complete!")

# Save model
print("ğŸ’¾ Saving Medarion model...")
trainer.save_model()
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"âœ… Model saved to: {OUTPUT_DIR}")

# Test Medarion
def test_medarion():
    print("ğŸ§ª Testing Medarion identity...")
    test_prompt = "### Instruction:\\nWhat is your name and what do you do?\\n\\n### Response:\\n"
    inputs = tokenizer(test_prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=200, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("ğŸ¤– Medarion Response:")
    print(response)

test_medarion()

print("ğŸ‰ Medarion training complete!")
print("âœ… Your 4GB Medarion AI is ready!")
print(f"ğŸ“ Download your model from: {OUTPUT_DIR}")
print("ğŸš€ This model will be faster and more efficient for your platform!")
