#!/usr/bin/env python3
"""
üöÄ Medarion QLoRA Fine-tuning Script - KAGGLE FREE TIER OPTIMIZED
================================================================

This version is specifically optimized for Kaggle's free tier:
- ‚úÖ Works within 30GB RAM limit
- ‚úÖ Optimized for T4 GPU (16GB VRAM)
- ‚úÖ 9-hour session limit compliance
- ‚úÖ Efficient memory usage
- ‚úÖ Faster training with smaller datasets
"""

# ============================================================
# üì¶ PACKAGE INSTALLATION & SETUP
# ============================================================
print("üì¶ Installing required packages for Kaggle free tier...")
import subprocess
import sys
import os
import json
import pickle
from datetime import datetime

try:
    subprocess.check_call([
        sys.executable, "-m", "pip", "install", 
        "transformers", "torch", "accelerate", "peft", "bitsandbytes", 
        "datasets", "psutil", "safetensors", "sentencepiece",
        "--no-cache-dir", "--quiet"
    ])
    print("‚úÖ Packages installed successfully!")
except Exception as e:
    print(f"‚ö†Ô∏è Package installation warning: {e}")

# Import required modules
import warnings
import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, 
    Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model, PeftModel
import psutil

# Suppress warnings and set environment variables
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256"  # Smaller chunks for free tier

# ============================================================
# üìÅ CONFIGURATION & PATHS - FREE TIER OPTIMIZED
# ============================================================
MODEL_NAME = "teknium/OpenHermes-2.5-Mistral-7B"
DATASET_PATH = "/kaggle/input/xone-finetuning-data"
OUTPUT_DIR = "/kaggle/working/medarion-mistral-qlora"
CHECKPOINT_DIR = "/kaggle/working/checkpoints"

# Create directories
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

print("üöÄ Starting Medarion QLoRA Fine-tuning (Kaggle Free Tier Optimized)...")
print(f"üìÇ Model: {MODEL_NAME}")
print(f"üìÇ Dataset: {DATASET_PATH}")
print(f"üìÇ Output: {OUTPUT_DIR}")
print(f"üìÇ Checkpoints: {CHECKPOINT_DIR}")

# ============================================================
# üîÑ PROGRESS TRACKING & RESUME FUNCTIONALITY
# ============================================================
def save_progress(step, status, data=None):
    """Save progress to resume from interruptions"""
    progress = {
        "step": step,
        "status": status,
        "timestamp": datetime.now().isoformat(),
        "data": data or {}
    }
    with open(f"{CHECKPOINT_DIR}/training_progress.json", "w") as f:
        json.dump(progress, f, indent=2)
    print(f"üíæ Progress saved: {step} - {status}")

def load_progress():
    """Load progress to resume from interruptions"""
    try:
        with open(f"{CHECKPOINT_DIR}/training_progress.json", "r") as f:
            return json.load(f)
    except FileNotFoundError:
        return {"step": 0, "status": "starting"}

def save_checkpoint(obj, filename):
    """Save Python objects for resuming"""
    try:
        with open(f"{CHECKPOINT_DIR}/{filename}", "wb") as f:
            pickle.dump(obj, f)
        print(f"üíæ Checkpoint saved: {filename}")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not save checkpoint {filename}: {e}")

def load_checkpoint(filename):
    """Load Python objects for resuming"""
    try:
        with open(f"{CHECKPOINT_DIR}/{filename}", "rb") as f:
            return pickle.load(f)
    except FileNotFoundError:
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è Could not load checkpoint {filename}: {e}")
        return None

# ============================================================
# üß† STEP 1: MODEL LOADING - FREE TIER OPTIMIZED
# ============================================================
progress = load_progress()
resume_from = progress.get("step", 0)

if resume_from <= 1:
    save_progress(1, "loading_model")
    print("üß† Step 1: Loading Model and Tokenizer (Free Tier Optimized)...")
    print("üì• Downloading OpenHermes 2.5 Mistral 7B model...")
    print("üí° Using 4-bit quantization for memory efficiency...")
    
    try:
        # Load tokenizer
        print("üîÑ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        tokenizer.pad_token = tokenizer.eos_token
        print("‚úÖ Tokenizer loaded successfully!")
        
        # Load model with 4-bit quantization - FREE TIER OPTIMIZED
        print("üîÑ Loading model in 4-bit precision...")
        print("üí° This may take 2-3 minutes...")
        
        # Configure 4-bit quantization for free tier
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=False,
            torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency
        )
        print("‚úÖ Model loaded successfully in 4-bit mode!")
        print(f"üìä Model size: {sum(p.numel() for p in model.parameters()):,} parameters")
        
        # Save checkpoints
        save_checkpoint(tokenizer, "tokenizer.pkl")
        save_checkpoint(model, "model.pkl")
        save_progress(1, "model_loaded")
        
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        save_progress(1, "model_loading_failed", {"error": str(e)})
        raise e
else:
    print("üîÑ Resuming from checkpoint...")
    tokenizer = load_checkpoint("tokenizer.pkl")
    model = load_checkpoint("model.pkl")
    if tokenizer is None or model is None:
        print("‚ùå Could not load model checkpoints, restarting...")
        save_progress(0, "restarting")
        raise Exception("Checkpoint loading failed")

# ============================================================
# ‚öôÔ∏è STEP 2: LoRA CONFIGURATION - FREE TIER OPTIMIZED
# ============================================================
if resume_from <= 2:
    save_progress(2, "setting_up_lora")
    print("‚öôÔ∏è Step 2: Setting up LoRA Configuration (Free Tier Optimized)...")
    print("üîß Configuring LoRA adapters for efficient training...")
    
    try:
        # FREE TIER OPTIMIZED LoRA config - smaller rank for memory efficiency
        lora_config = LoraConfig(
            r=8,  # Reduced from 16 for free tier
            lora_alpha=16,  # Reduced from 32 for free tier
            target_modules=["q_proj", "v_proj"],  # Fewer modules for free tier
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        print("üîÑ Applying LoRA adapters to model...")
        model = get_peft_model(model, lora_config)
        print("‚úÖ LoRA adapters attached successfully!")
        print(f"üìä Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
        
        save_checkpoint(model, "lora_model.pkl")
        save_progress(2, "lora_configured")
        
    except Exception as e:
        print(f"‚ùå Error setting up LoRA: {e}")
        save_progress(2, "lora_setup_failed", {"error": str(e)})
        raise e
else:
    print("üîÑ Loading LoRA model from checkpoint...")
    model = load_checkpoint("lora_model.pkl")
    if model is None:
        print("‚ùå Could not load LoRA model checkpoint")
        raise Exception("LoRA checkpoint loading failed")

# ============================================================
# üìä STEP 3: DATA LOADING - FREE TIER OPTIMIZED
# ============================================================
if resume_from <= 3:
    save_progress(3, "loading_data")
    print("üìä Step 3: Loading Training Data (Free Tier Optimized)...")
    print("üìÇ Loading dataset files...")
    
    try:
        def load_jsonl(path, max_samples=None):
            """Load JSONL with optional sample limit for free tier"""
            data = []
            print(f"üîÑ Loading {path}...")
            with open(path, "r", encoding="utf-8") as f:
                for i, line in enumerate(f):
                    if max_samples and i >= max_samples:
                        break
                    if i % 50000 == 0:
                        print(f"üìä Loaded {i:,} records...")
                    data.append(json.loads(line))
            return data
        
        # FREE TIER OPTIMIZATION: Use smaller dataset for faster training
        print("üîÑ Loading training data (FREE TIER: Using 100K samples for faster training)...")
        train_data = load_jsonl(f"{DATASET_PATH}/train.jsonl", max_samples=100000)
        print(f"‚úÖ Loaded {len(train_data):,} training records (FREE TIER OPTIMIZED)")
        
        print("üîÑ Loading validation data (FREE TIER: Using 10K samples)...")
        val_data = load_jsonl(f"{DATASET_PATH}/validation.jsonl", max_samples=10000)
        print(f"‚úÖ Loaded {len(val_data):,} validation records (FREE TIER OPTIMIZED)")
        
        # Save data checkpoints
        save_checkpoint(train_data, "train_data.pkl")
        save_checkpoint(val_data, "val_data.pkl")
        save_progress(3, "data_loaded")
        
    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        save_progress(3, "data_loading_failed", {"error": str(e)})
        raise e
else:
    print("üîÑ Loading data from checkpoints...")
    train_data = load_checkpoint("train_data.pkl")
    val_data = load_checkpoint("val_data.pkl")
    if train_data is None or val_data is None:
        print("‚ùå Could not load data checkpoints")
        raise Exception("Data checkpoint loading failed")

# ============================================================
# üîÑ STEP 4: DATA FORMATTING - FREE TIER OPTIMIZED
# ============================================================
if resume_from <= 4:
    save_progress(4, "formatting_data")
    print("üîÑ Step 4: Formatting and Tokenizing Data (Free Tier Optimized)...")
    print("üí° This may take 5-10 minutes...")
    
    try:
        # Clear memory before processing
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        def format_example(example):
            if example.get("input"):
                text = (
                    f"### Instruction:\n{example['instruction']}\n\n"
                    f"### Input:\n{example['input']}\n\n"
                    f"### Response:\n{example['output']}"
                )
            else:
                text = (
                    f"### Instruction:\n{example['instruction']}\n\n"
                    f"### Response:\n{example['output']}"
                )
            return {"text": text}
        
        def tokenize_function(examples):
            # Tokenize the text
            tokenized = tokenizer(
                examples["text"],
                truncation=True,
                padding=False,  # Let data collator handle padding
                max_length=512,  # FREE TIER: Reduced from 1024 for memory efficiency
                return_tensors=None
            )
            # Return only the tokenizer outputs, remove any extra columns
            return {
                "input_ids": tokenized["input_ids"],
                "attention_mask": tokenized["attention_mask"]
            }
        
        print("üîÑ Formatting training data...")
        print(f"üìä Processing {len(train_data):,} training records...")
        train_dataset = Dataset.from_list(train_data).map(
            format_example, 
            num_proc=None,  # Disable multiprocessing completely
            desc="Formatting training data"
        )
        print("‚úÖ Training data formatted!")
        
        print("üîÑ Formatting validation data...")
        print(f"üìä Processing {len(val_data):,} validation records...")
        val_dataset = Dataset.from_list(val_data).map(
            format_example, 
            num_proc=None,  # Disable multiprocessing completely
            desc="Formatting validation data"
        )
        print("‚úÖ Validation data formatted!")
        
        print("üîÑ Tokenizing training data...")
        train_dataset = train_dataset.map(
            tokenize_function, 
            batched=True, 
            remove_columns=["text", "instruction", "input", "output"],  # Remove ALL original columns
            num_proc=None,  # Disable multiprocessing completely
            desc="Tokenizing training data"
        )
        print("‚úÖ Training data tokenized!")
        
        print("üîÑ Tokenizing validation data...")
        val_dataset = val_dataset.map(
            tokenize_function, 
            batched=True, 
            remove_columns=["text", "instruction", "input", "output"],  # Remove ALL original columns
            num_proc=None,  # Disable multiprocessing completely
            desc="Tokenizing validation data"
        )
        print("‚úÖ Validation data tokenized!")
        
        # Save dataset checkpoints
        save_checkpoint(train_dataset, "train_dataset.pkl")
        save_checkpoint(val_dataset, "val_dataset.pkl")
        save_progress(4, "data_formatted")
        
    except Exception as e:
        print(f"‚ùå Error formatting data: {e}")
        save_progress(4, "formatting_failed", {"error": str(e)})
        raise e
else:
    print("üîÑ Loading formatted datasets from checkpoints...")
    train_dataset = load_checkpoint("train_dataset.pkl")
    val_dataset = load_checkpoint("val_dataset.pkl")
    if train_dataset is None or val_dataset is None:
        print("‚ùå Could not load dataset checkpoints")
        raise Exception("Dataset checkpoint loading failed")

# ============================================================
# ‚öôÔ∏è STEP 5: TRAINING SETUP - FREE TIER OPTIMIZED
# ============================================================
if resume_from <= 5:
    save_progress(5, "setting_up_training")
    print("‚öôÔ∏è Step 5: Setting up Training Configuration (Free Tier Optimized)...")
    print("üîß Configuring training arguments for QLoRA...")
    
    try:
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer, 
            mlm=False,
            pad_to_multiple_of=8,
            return_tensors="pt"
        )
        
        # FREE TIER OPTIMIZED training arguments
        training_args = TrainingArguments(
            output_dir=OUTPUT_DIR,
            num_train_epochs=2,  # FREE TIER: Reduced from 3 for faster training
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=8,  # FREE TIER: Increased for effective larger batch
            eval_strategy="steps",
            eval_steps=200,  # FREE TIER: More frequent evaluation
            save_strategy="steps",
            save_steps=500,  # FREE TIER: More frequent saves
            save_total_limit=2,
            fp16=True,
            learning_rate=3e-5,  # FREE TIER: Slightly higher LR for faster convergence
            warmup_steps=50,  # FREE TIER: Reduced warmup
            weight_decay=0.01,
            logging_steps=25,  # FREE TIER: More frequent logging
            report_to="none",
            dataloader_num_workers=0,
            dataloader_drop_last=True,
            max_grad_norm=1.0,
            save_safetensors=True,
            remove_unused_columns=False,
            run_name="medarion-qlora-free-tier",
            # FREE TIER: Additional optimizations
            dataloader_pin_memory=False,  # Save memory
            dataloader_persistent_workers=False,  # Save memory
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
        )
        
        print("‚úÖ Training configuration complete!")
        print(f"üìä Training dataset: {len(trainer.train_dataset):,} records")
        print(f"üìä Evaluation dataset: {len(trainer.eval_dataset):,} records")
        
        # Debug: Check dataset format
        print("üîç Debug: Checking dataset format...")
        sample = train_dataset[0]
        print(f"üìä Sample keys: {list(sample.keys())}")
        print(f"üìä Input IDs type: {type(sample.get('input_ids', 'Not found'))}")
        print(f"üìä Attention mask type: {type(sample.get('attention_mask', 'Not found'))}")
        if 'input_ids' in sample:
            print(f"üìä Input IDs length: {len(sample['input_ids'])}")
        if 'attention_mask' in sample:
            print(f"üìä Attention mask length: {len(sample['attention_mask'])}")
        
        save_checkpoint(trainer, "trainer.pkl")
        save_progress(5, "training_configured")
        
    except Exception as e:
        print(f"‚ùå Error setting up training: {e}")
        save_progress(5, "training_setup_failed", {"error": str(e)})
        raise e
else:
    print("üîÑ Loading trainer from checkpoint...")
    trainer = load_checkpoint("trainer.pkl")
    if trainer is None:
        print("‚ùå Could not load trainer checkpoint")
        raise Exception("Trainer checkpoint loading failed")

# ============================================================
# üîç STEP 6: PRE-TRAINING DIAGNOSTICS
# ============================================================
if resume_from <= 6:
    save_progress(6, "running_diagnostics")
    print("üîç Step 6: Pre-Training Diagnostics and Safety Checks...")
    
    try:
        # Check 1: GPU Status
        print("üîç Check 1: GPU Status...")
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"‚úÖ GPU available: {gpu_count} device(s)")
            print(f"‚úÖ GPU memory: {gpu_memory:.1f} GB")
            torch.cuda.empty_cache()
        else:
            print("‚ö†Ô∏è No GPU available, using CPU")
        
        # Check 2: Model Status
        print("üîç Check 2: Model Status...")
        model_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"‚úÖ Model parameters: {model_params:,}")
        print(f"‚úÖ Trainable parameters: {trainable_params:,}")
        print(f"‚úÖ Trainable ratio: {trainable_params/model_params*100:.2f}%")
        
        # Check 3: Data Status
        print("üîç Check 3: Data Status...")
        print(f"‚úÖ Training samples: {len(train_dataset):,}")
        print(f"‚úÖ Validation samples: {len(val_dataset):,}")
        print(f"‚úÖ Tokenizer vocab size: {len(tokenizer):,}")
        
        # Check 4: System Resources
        print("üîç Check 4: System Resources...")
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        print(f"‚úÖ CPU usage: {cpu_percent}%")
        print(f"‚úÖ RAM usage: {memory.percent}% ({memory.used/1e9:.1f}GB / {memory.total/1e9:.1f}GB)")
        
        if cpu_percent > 90:
            print("‚ö†Ô∏è High CPU usage detected")
        if memory.percent > 90:
            print("‚ö†Ô∏è High memory usage detected")
        
        # Check 5: Training Configuration
        print("üîç Check 5: Training Configuration...")
        print(f"‚úÖ Batch size: {training_args.per_device_train_batch_size}")
        print(f"‚úÖ Gradient accumulation: {training_args.gradient_accumulation_steps}")
        print(f"‚úÖ Learning rate: {training_args.learning_rate}")
        print(f"‚úÖ Epochs: {training_args.num_train_epochs}")
        
        # Check 6: Forward Pass Test
        print("üîç Check 6: Forward Pass Test...")
        try:
            sample_data = train_dataset.select(range(min(2, len(train_dataset))))
            sample_batch = data_collator([sample_data[i] for i in range(len(sample_data))])
            
            if torch.cuda.is_available():
                sample_batch = {k: v.to('cuda') if isinstance(v, torch.Tensor) else v for k, v in sample_batch.items()}
            
            with torch.no_grad():
                outputs = model(**sample_batch)
            print("‚úÖ Forward pass test successful")
        except Exception as e:
            print(f"‚ö†Ô∏è Forward pass test failed: {e}")
            print("üí° This might be due to tokenization issues, but training should still work")
        
        print("‚úÖ All diagnostics completed!")
        save_progress(6, "diagnostics_complete")
        
    except Exception as e:
        print(f"‚ùå Diagnostics failed: {e}")
        save_progress(6, "diagnostics_failed", {"error": str(e)})
        print("üí° Continuing with training despite diagnostic issues...")

# ============================================================
# üöÄ STEP 7: TRAINING - FREE TIER OPTIMIZED
# ============================================================
if resume_from <= 7:
    save_progress(7, "starting_training")
    print("üöÄ Step 7: Starting QLoRA Training (Free Tier Optimized)...")
    print("üí° This will take 2-3 hours (FREE TIER OPTIMIZED)...")
    print("üìä Training on 100K records with 10K validation records")
    print("üîÑ Using QLoRA for efficient training...")
    
    try:
        print("üîÑ Beginning training...")
        trainer.train(resume_from_checkpoint=None)
        print("‚úÖ Training completed successfully!")
        save_progress(7, "training_complete")
        
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        save_progress(7, "training_failed", {"error": str(e)})
        raise e
else:
    print("üîÑ Resuming training from checkpoint...")
    try:
        trainer.train(resume_from_checkpoint=True)
        print("‚úÖ Training completed successfully!")
        save_progress(7, "training_complete")
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        save_progress(7, "training_failed", {"error": str(e)})
        raise e

# ============================================================
# üíæ STEP 8: SAVE MODEL
# ============================================================
if resume_from <= 8:
    save_progress(8, "saving_model")
    print("üíæ Step 8: Saving LoRA Adapter...")
    
    try:
        print("üîÑ Saving LoRA adapter...")
        model.save_pretrained(OUTPUT_DIR)
        tokenizer.save_pretrained(OUTPUT_DIR)
        print("‚úÖ LoRA adapter saved successfully!")
        
        save_progress(8, "adapter_saved")
        
    except Exception as e:
        print(f"‚ùå Error saving adapter: {e}")
        save_progress(8, "adapter_save_failed", {"error": str(e)})
        raise e

# ============================================================
# üîÑ STEP 9: MERGE MODEL - FREE TIER OPTIMIZED
# ============================================================
if resume_from <= 9:
    save_progress(9, "merging_model")
    print("üîÑ Step 9: Merging LoRA Adapter into Full Model (Free Tier Optimized)...")
    print("üí° This may take 3-5 minutes...")
    
    try:
        print("üîÑ Loading base model for merging...")
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME, 
            torch_dtype=torch.float16,  # FREE TIER: Use float16 for memory efficiency
            low_cpu_mem_usage=True,
            device_map="cpu"  # Use CPU for merging to save GPU memory
        )
        
        print("üîÑ Loading LoRA adapter...")
        peft_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)
        
        print("üîÑ Merging adapter into base model...")
        merged_model = peft_model.merge_and_unload()
        
        print("üîÑ Saving merged model...")
        merged_model.save_pretrained(f"{OUTPUT_DIR}/merged_model", safe_serialization=True)
        tokenizer.save_pretrained(f"{OUTPUT_DIR}/merged_model")
        
        print("‚úÖ Merged model saved successfully!")
        save_progress(9, "model_merged")
        
    except Exception as e:
        print(f"‚ùå Error merging model: {e}")
        save_progress(9, "model_merge_failed", {"error": str(e)})
        raise e

# ============================================================
# üß™ STEP 10: TEST MODEL
# ============================================================
if resume_from <= 10:
    save_progress(10, "testing_model")
    print("üß™ Step 10: Testing Fine-tuned Model...")
    
    try:
        from transformers import pipeline
        
        print("üîÑ Loading model for testing...")
        pipe = pipeline(
            "text-generation", 
            model=f"{OUTPUT_DIR}/merged_model", 
            tokenizer=tokenizer, 
            device_map="auto",
            torch_dtype=torch.float16  # FREE TIER: Use float16 for memory efficiency
        )
        
        # Test 1: Identity
        print("üß™ Test 1: Identity Check...")
        prompt1 = "### Instruction:\nWhat is your name and what do you specialize in?\n\n### Response:\n"
        result1 = pipe(prompt1, max_new_tokens=100, temperature=0.7, do_sample=True)
        print(f"ü§ñ Response: {result1[0]['generated_text'][len(prompt1):]}")
        
        # Test 2: Healthcare Knowledge
        print("üß™ Test 2: Healthcare Knowledge...")
        prompt2 = "### Instruction:\nWhat are the key considerations for clinical trial design?\n\n### Response:\n"
        result2 = pipe(prompt2, max_new_tokens=150, temperature=0.7, do_sample=True)
        print(f"ü§ñ Response: {result2[0]['generated_text'][len(prompt2):]}")
        
        print("‚úÖ Model testing completed!")
        save_progress(10, "model_tested")
        
    except Exception as e:
        print(f"‚ùå Error testing model: {e}")
        save_progress(10, "model_test_failed", {"error": str(e)})
        print("üí° Model may still be usable despite test failure")

# ============================================================
# üì¶ STEP 11: CREATE DOWNLOAD PACKAGE
# ============================================================
if resume_from <= 11:
    save_progress(11, "creating_package")
    print("üì¶ Step 11: Creating Download Package...")
    
    try:
        print("üîÑ Creating ZIP package...")
        import subprocess
        result = subprocess.run([
            "zip", "-r", "/kaggle/working/medarion_final_model.zip", 
            f"{OUTPUT_DIR}/merged_model"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ Download package created successfully!")
            print("üìÅ File: /kaggle/working/medarion_final_model.zip")
            
            # Get file size
            import os
            file_size = os.path.getsize("/kaggle/working/medarion_final_model.zip") / 1e9
            print(f"üìä Package size: {file_size:.2f} GB")
            
            save_progress(11, "package_created")
        else:
            print(f"‚ùå Error creating package: {result.stderr}")
            save_progress(11, "package_creation_failed", {"error": result.stderr})
            
    except Exception as e:
        print(f"‚ùå Error creating package: {e}")
        save_progress(11, "package_creation_failed", {"error": str(e)})

# ============================================================
# üéâ COMPLETION
# ============================================================
print("\nüéâ Medarion QLoRA Fine-tuning Complete (Kaggle Free Tier)!")
print("=" * 60)
print("‚úÖ Model trained successfully with QLoRA")
print("‚úÖ LoRA adapter saved")
print("‚úÖ Model merged and ready for deployment")
print("‚úÖ Model tested and validated")
print("‚úÖ Download package created")
print("\nüìÅ Files available:")
print(f"   ‚Ä¢ LoRA Adapter: {OUTPUT_DIR}")
print(f"   ‚Ä¢ Merged Model: {OUTPUT_DIR}/merged_model")
print(f"   ‚Ä¢ Download ZIP: /kaggle/working/medarion_final_model.zip")
print("\nüöÄ Your Medarion AI model is ready for deployment!")
print("üí° FREE TIER OPTIMIZED: Trained on 100K samples in 2-3 hours")

# Final progress save
save_progress(12, "complete")
